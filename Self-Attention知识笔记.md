# Self-Attention 知识笔记

> 从 RoPE + RMSNorm 之后，Transformer 如何通过 Self-Attention 学习前文信息

---

## 一、核心概念与位置

### Self-Attention 在 Transformer 中的角色

```
Transformer Block 完整流程：

输入 x [seqlen, d_model]
  ↓
RMSNorm(x)  ← 【已学：对输入归一化，稳定训练】
  ↓
RoPE(x)     ← 【已学：添加旋转位置编码，编码相对位置】
  ↓
Self-Attention ← 【本章：让每个词学到前文的关键信息】
  ↓
残差连接 + 第二个 RMSNorm
  ↓
FFN（Feed-Forward Network）
  ↓
输出 y [seqlen, d_model]
```

**核心作用**：给定前文，通过注意力机制聚合相关的历史信息，为生成下一个词提供"上下文"。

---

## 二、Q、K、V 的来源与含义

### 2.1 线性投影（来自学习的权重矩阵）

Self-Attention 的输入是经过 RoPE 处理的向量 `x`，它包含了位置信息。接下来通过三个**可学习的线性变换**生成 Q、K、V：

```
x [seqlen, d_model=4096]  ← 每个词已编码位置信息
  ├─→ 乘以 W_q [4096, d=128]  → Q [seqlen, nhead, d]
  ├─→ 乘以 W_k [4096, d=128]  → K [total_len, nkvhead, d]
  └─→ 乘以 W_v [4096, dv=128] → V [total_len, nkvhead, dv]
```

**关键点**：虽然 Q、K、V 都来自同一个 x，但它们经过**不同的权重矩阵**，因此突出了 x 的不同方面。

### 2.2 参数从何而来？

**W_q、W_k、W_v 都是神经网络的可学习参数！**

#### 初始化阶段
```
模型创建时：
  W_q ← 随机初始化（正态分布）
  W_k ← 随机初始化
  W_v ← 随机初始化
```

#### 训练阶段（反向传播）

```
每次训练迭代：

前向传播：
  Q = x @ W_q
  K = x @ W_k
  V = x @ W_v  ← 注意：这里也会被优化
  attn_output = softmax(Q @ K.T / sqrt(d)) @ V
  pred = FFN(attn_output)

计算损失：
  loss = CrossEntropy(pred, 真实标签)

反向传播梯度链（非常重要）：
  loss
    ↓ (从后续层反向传播回来)
  attn_output的梯度
    ↓ (softmax_weights @ V 的梯度分解为两部分)
  ├─→ softmax_weights的梯度
  │     ↓
  │   Q @ K^T 的梯度  ← 传给W_q、W_k
  │
  └─→ V的梯度
        ↓
      x @ W_v 的梯度  ← 传给W_v！
      
所以，W_v也有梯度和更新！

参数更新（梯度下降）：
  ∂loss/∂W_q = (attention梯度通过Q) @ x.T
  ∂loss/∂W_k = (attention梯度通过K) @ x.T
  ∂loss/∂W_v = (value聚合梯度通过V) @ x.T  ← W_v也改变！
  
  W_q_new = W_q_old - learning_rate × ∂loss/∂W_q
  W_k_new = W_k_old - learning_rate × ∂loss/∂W_k
  W_v_new = W_v_old - learning_rate × ∂loss/∂W_v
```

**三个权重矩阵都被同时训练！**

- **W_q** 学会：如何在"查询空间"中突出需要什么信息
- **W_k** 学会：如何在"键空间"中突出是什么信息（与Q匹配）
- **W_v** 学会：如何提取什么样的"值向量"来被聚合

**经过数百万个迭代后**，W_q、W_k、W_v 逐渐学会了提取和匹配语义特征。

### 2.3 Q、K、V 的语义含义

```
Q（Query，查询）：
  ├─ 含义：当前词"我需要什么信息？"
  ├─ 作用：在"查询特征空间"中的投影
  ├─ 形状：[seqlen, nhead, d]
  └─ 用途：用来与其他词的K比对

K（Key，键）：
  ├─ 含义：前文词"我是什么？"
  ├─ 作用：在"键特征空间"中的投影
  ├─ 形状：[total_len, nkvhead, d]  (可能包含KV Cache)
  └─ 用途：被Q查询匹配

V（Value，值）：
  ├─ 含义：前文词"我的信息是什么？"
  ├─ 作用：在"值特征空间"中的投影
  ├─ 形状：[total_len, nkvhead, dv]
  └─ 用途：被加权聚合
```

---

## 三、Self-Attention 的数学原理

### 3.1 核心公式

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right) V$$

### 3.2 各步骤详解

#### Step 1：计算相似度矩阵

$$A = \frac{QK^T}{\sqrt{d}}$$

```
形状计算：
Q [seqlen, d] × K^T [d, total_len]  = A [seqlen, total_len]

含义：
A[i, j] = (Q[i] · K[j]) / sqrt(d)
        = 第i个词的查询与第j个词的键的相似度（归一化）

例子（seqlen=3, total_len=3）：
A = [[Q0·K0, Q0·K1, Q0·K2],   ← 词0的查询与所有词的键相似度
     [Q1·K0, Q1·K1, Q1·K2],   ← 词1的查询与所有词的键相似度
     [Q2·K0, Q2·K1, Q2·K2]]   ← 词2的查询与所有词的键相似度
```

**为什么点积能衡量相似度？**

```
两个向量的点积反映它们的"对齐程度"：

v1 = [1, 0]（指向东），v2 = [1, 0]（也指向东）
v1 · v2 = 1  ← 完全相同，相似度最大

v3 = [0, 1]（指向北）
v1 · v3 = 0  ← 垂直，毫无关联

在Self-Attention中：
  训练时，W_q和W_k会学到在"查询空间"和"键空间"中
  都强调语义关键特征的投影方式
  
  → Q·K^T 自动发现语义相关的词对
  → 相关词的相似度高，不相关词的相似度低
```

#### Step 2：应用 Causal Mask（因果掩码）

```
生成任务必须是因果的：第i个词只能看到前i个词（包括自己）

Causal Mask 矩阵：
     词0  词1  词2
词0   ✓    ✗    ✗     可以看   不能看（未来）
词1   ✓    ✓    ✗
词2   ✓    ✓    ✓

实现：将未来位置的分数设为 -∞

A_masked = A + Mask
         
如果 Mask[i, j] = 0（i能看j），保持原值
如果 Mask[i, j] = -∞（i不能看j），设为 -∞

例如：
A_masked[0] = [0.5, -∞, -∞]  ← 词0只关注自己
A_masked[2] = [0.3, 0.2, 0.4]  ← 词2能看全部
```

#### Step 3：Softmax 归一化

$$\text{Attention Weights} = \text{softmax}(A_{masked})$$

```
对每一行独立做softmax（将分数转换为概率）：

weights[i] = exp(A_masked[i]) / sum(exp(A_masked[i]))

例子：
A_masked[2] = [0.3, 0.2, 0.4]

exp(values) = [e^0.3, e^0.2, e^0.4] = [1.35, 1.22, 1.49]
sum = 4.06

weights[2] = [1.35/4.06, 1.22/4.06, 1.49/4.06]
           = [0.33, 0.30, 0.37]

性质：
  ✓ 所有元素都是非负数
  ✓ 每一行的和为 1.0（概率分布）
  ✓ 最高的值对应"最需要关注"的词
```

#### Step 4：加权求和聚合信息

$$\text{Output} = \text{Attention Weights} \times V$$

```
形状计算：
Attention Weights [seqlen, total_len] × V [total_len, dv]
  = Output [seqlen, dv]

含义：
output[i] = sum(weights[i, j] * V[j] for j in range(total_len))
          = 用注意力权重加权求和所有词的value向量

例子：
output[2] = 0.33 * V[0] + 0.30 * V[1] + 0.37 * V[2]

              词0的值        词1的值        词2的值
           (权重0.33)    (权重0.30)    (权重0.37)

结果：output[2] 是一个融合了三个词信息的向量
      其中词2的信息最多（权重0.37），词0次之（0.33）
```

---

## 四、完整工作流程示例

### 场景：翻译模型生成"鱼"

**输入**："猫 喜欢 吃 ___"  
**目标**：生成"鱼"（或其他食物）

### 前向推理过程

```
词序列：  猫    喜欢   吃
位置ID：  0     1      2

Step 1：Embedding + RoPE
  x[0] = embedding("猫") with RoPE(pos=0)    [d_model=4096]
  x[1] = embedding("喜欢") with RoPE(pos=1)  [d_model=4096]
  x[2] = embedding("吃") with RoPE(pos=2)    [d_model=4096]

Step 2：投影到Q、K、V空间
  Q = x @ W_q  [3, 32头, 128维]
  K = x @ W_k  [3, 8头, 128维]   (GQA: 8个共享头)
  V = x @ W_v  [3, 8头, 128维]
  
  详细值（简化为3维）：
  Q[0]"猫" = [0.3, 0.1, 0.5]
  Q[1]"喜欢" = [0.2, 0.4, 0.1]
  Q[2]"吃" = [0.8, 0.3, 0.2]
  
  K[0]"猫" = [0.25, 0.15, 0.5]
  K[1]"喜欢" = [0.22, 0.38, 0.15]
  K[2]"吃" = [0.75, 0.35, 0.25]
  
  V[0]"猫" = [值向量...]
  V[1]"喜欢" = [值向量...]
  V[2]"吃" = [值向量...]

Step 3：计算相似度
  A = Q @ K^T / sqrt(128)
  
  A[0] = [0.34, 0.28, 0.25]  ← Q[0]与所有K的相似度
  A[1] = [0.29, 0.32, 0.31]
  A[2] = [0.58, 0.35, 0.52]  ← Q[2]与所有K的相似度
  
  关键观察：
    A[2, 0] = 0.58  ← "吃"的查询与"猫"的键高度相似！
    （模型学到了"动作"与"主体"的强关联）

Step 4：应用Causal Mask
  A_masked[0] = [0.34, -∞, -∞]
  A_masked[1] = [0.29, 0.32, -∞]
  A_masked[2] = [0.58, 0.35, 0.52]

Step 5：Softmax
  weights[0] = softmax([0.34, -∞, -∞]) = [1.0, 0, 0]
  weights[1] = softmax([0.29, 0.32, -∞]) ≈ [0.48, 0.52, 0]
  weights[2] = softmax([0.58, 0.35, 0.52]) ≈ [0.40, 0.24, 0.36]
  
  → "吃"关注"猫"的权重：0.40（最高！）
  → "吃"关注"喜欢"的权重：0.24
  → "吃"关注自己的权重：0.36

Step 6：加权聚合
  output[2] = 0.40 * V[0]"猫" + 0.24 * V[1]"喜欢" + 0.36 * V[2]"吃"
            = 融合向量，主要包含"猫"和"吃"的信息
  
  这个向量已经"知道"：
    - 主体是"猫"（权重0.40）
    - 猫喜欢做某事（权重0.24）
    - 正在做"吃"的动作（权重0.36）

Step 7：后续层处理
  output[2] 传给 FFN 和其他Transformer层
  最终输出 logits，解码得到 "鱼"
```

**这就是模型"知道"前文信息的原理！**

---

## 五、GQA（Grouped Query Attention）与 KV Cache

### 5.1 为什么要用 GQA？

```
标准 Multi-Head Attention（MHA）：
  Q: [seqlen, nhead=32, d]
  K: [seqlen, nhead=32, d]
  V: [seqlen, nhead=32, d]
  
  内存占用：大！每个Q头都有独立的K、V头

Grouped Query Attention（GQA）：
  Q: [seqlen, nhead=32, d]       ← 仍然32个查询头
  K: [seqlen, nkvhead=8, d]      ← 只有8个键头（共享）
  V: [seqlen, nkvhead=8, d]      ← 只有8个值头（共享）
  
  映射关系（重要）：
  多个Q头共享一个K/V头
  
  K/V_head_idx = Q_head_idx // 4  (每4个Q头共享1个K/V头)
  
  具体映射：
  ┌─────────── Q头0 ┐
  ├─────────── Q头1 ├──→ K/V头0  (共享同一个K/V)
  ├─────────── Q头2 │
  ├─────────── Q头3 ┘
  ├─────────── Q头4 ┐
  ├─────────── Q头5 ├──→ K/V头1  (共享同一个K/V)
  ├─────────── Q头6 │
  ├─────────── Q头7 ┘
  ...
  
  效果分析：
  ✓ Q头0-3都使用K/V头0，但用不同的查询方式
  ✓ 这4个头从同一个"值向量"中提取不同的特征
  ✓ KV Cache 内存减少 75%（32→8）
  ✓ 计算量没有显著增加，但推理速度更快
```

#### GQA 与特征学习的关系

```
不同Q头学习不同的查询模式：

Q头0: "词性匹配" 查询空间
Q头1: "语义相似" 查询空间
Q头2: "距离关系" 查询空间
Q头3: "实体指代" 查询空间
↓ (都使用) K/V头0

K/V头0: 提供某种"基础特征值"
  ├─ 被Q头0用"词性匹配"的视角查询
  ├─ 被Q头1用"语义相似"的视角查询
  ├─ 被Q头2用"距离关系"的视角查询
  └─ 被Q头3用"实体指代"的视角查询

结果：
  同一个V向量被4种不同的"眼光"审视
  → 4个头得到4种不同的解释
  → 最终聚合时融合这4种视角
  
这就是GQA的巧妙之处：
  用较少的值向量(8个)，通过多个查询视角(32个)，
  实现和MHA相当的特征丰富度，但内存占用更少！
```

#### 完整计算流程详解（重要！）

**以处理第3个词"吃"为例**：

```
输入：词"吃"的向量 x[2] [d_model=4096]

Step 1: 线性投影生成Q、K、V
  Q[2] = x[2] @ W_q  → [32头, 128维]
  K[2] = x[2] @ W_k  → [8头, 128维]
  V[2] = x[2] @ W_v  → [8头, 128维]

  展开Q（32个头）：
  Q[2] = [Q头0, Q头1, Q头2, ..., Q头31]  每个头128维

  展开K（8个头）：
  K[2] = [K头0, K头1, ..., K头7]  每个头128维

  展开V（8个头）：
  V[2] = [V头0, V头1, ..., V头7]  每个头128维

Step 2: 对每个Q头分别计算注意力（这里是关键！）

  Q头0（第0组）：
    ├─ 找到对应的K/V头：K/V头0（因为 0 // 4 = 0）
    ├─ 计算相似度：
    │   scores_0 = Q头0 @ [K头0[词0], K头0[词1], K头0[词2]]^T
    │            = [score_00, score_01, score_02]
    │            ← 与前文所有词的K头0比对
    ├─ Causal Softmax：
    │   weights_0 = softmax([score_00, score_01, score_02])
    │             = [0.3, 0.3, 0.4]
    └─ 加权求和V：
        output_0 = 0.3*V头0[词0] + 0.3*V头0[词1] + 0.4*V头0[词2]
                 = [128维融合向量]

  Q头1（第0组）：
    ├─ 对应K/V头：K/V头0（因为 1 // 4 = 0，共享同一个K/V）
    ├─ 计算相似度：
    │   scores_1 = Q头1 @ [K头0[词0], K头0[词1], K头0[词2]]^T
    │            = [score_10, score_11, score_12]
    │            ← 注意：用的是同一个K头0，但Q不同！
    ├─ Causal Softmax：
    │   weights_1 = softmax([score_10, score_11, score_12])
    │             = [0.5, 0.2, 0.3]  ← 与Q头0不同的权重！
    └─ 加权求和V：
        output_1 = 0.5*V头0[词0] + 0.2*V头0[词1] + 0.3*V头0[词2]
                 = [128维融合向量，但权重不同！]

  ... (Q头2、Q头3 也都用K/V头0)

  Q头4（第1组）：
    ├─ 对应K/V头：K/V头1（因为 4 // 4 = 1）
    ├─ 计算相似度：
    │   scores_4 = Q头4 @ [K头1[词0], K头1[词1], K头1[词2]]^T
    │            ← 用的是K头1，不同的K特征！
    └─ ...
  
  ... (重复32次，每个Q头都独立计算)

Step 3: 拼接所有头的输出
  final_output = concat(output_0, output_1, ..., output_31)
               = [32 × 128 = 4096维]
               ← 融合了32个头的信息

Step 4: 输出投影（可选）
  result = final_output @ W_out
         = [d_model=4096维]
         ← 最终的包含前文信息的向量！
```

**关键理解**：

```
✓ 每个Q头只与1个K/V头配对（不是8个）
  Q头0-3 → K/V头0
  Q头4-7 → K/V头1
  ...

✗ 不是"每个头与8个K/V运算"
  而是"32个Q头分成8组，每组用一个K/V"

✓ 虽然Q头0-3共享K/V头0，但因为Q不同：
  → Q头0·K头0 得到的相似度 ≠ Q头1·K头0 得到的相似度
  → 所以attention权重不同
  → 聚合的V向量权重不同
  → 4个输出虽然用同一个V，但加权不同，结果不同！

✓ 最终32个头输出拼接：
  [output_0 | output_1 | ... | output_31]
  = 包含32种不同视角的信息
  = 对"吃"这个词的完整上下文理解
```

**直观比喻**：

```
书店里有8本书（K/V头0-7）
你有32副不同的眼镜（Q头0-31）

戴眼镜0-3看书0：
  ├─ 眼镜0（词性视角）：看到"名词、动词、介词..."
  ├─ 眼镜1（语义视角）：看到"动物、食物、地点..."
  ├─ 眼镜2（距离视角）：看到"远、近、相邻..."
  └─ 眼镜3（指代视角）：看到"主体、客体、代词..."

戴眼镜4-7看书1：
  （又是4种不同的视角解读另一本书）

最终：32副眼镜 × 8本书 = 32种不同的理解
拼接起来 = 对当前词的完整上下文表示
```

### 5.2 KV Cache 加速推理

```
场景：生成一个1000词的文本

不用KV Cache（低效）：
  生成词1：计算完整序列的K、V [1, nhead, d]
  生成词2：重新计算 [2, nhead, d] ← 词1被重复计算！
  生成词3：重新计算 [3, nhead, d] ← 词1、2被重复计算！
  ...
  总计算量 ∝ 1000²

用KV Cache（高效）：
  生成词1：计算K1、V1，存入cache
  生成词2：只计算新K2、V2，拼接cache → K = [K1, K2]
  生成词3：只计算新K3、V3，拼接cache → K = [K1, K2, K3]
  ...
  总计算量 ∝ 1000  ← 线性而非平方！
  
实现：
  K_cache = [K1, K2, ..., K_{i-1}]  [已生成词数, nkvhead, d]
  V_cache = [V1, V2, ..., V_{i-1}]
  
  生成第i个词时：
    K_new = concat(K_cache, K_i)  [i, nkvhead, d]
    V_new = concat(V_cache, V_i)
    
    Attention(Q_i, K_new, V_new)  ← 完整的注意力机制
```

### 5.3 GQA 的性能-成本权衡

#### GQA 是否以牺牲性能换成本？

**简答**：有轻微性能损失，但权衡**极其划算**！

根据 GQA 论文的实验数据：

```
对比标准MHA（32个K/V头）：

GQA-8（8个K/V头）：
  ├─ 模型困惑度：↓ 0-2%  ← 几乎看不出来
  ├─ 推理速度：↑ 2-3倍
  ├─ KV Cache内存：↓ 75%
  └─ 性价比：★★★★★ 极高！

GQA-16（16个K/V头）：
  ├─ 困惑度：↓ 0%（基本无损失）
  └─ KV Cache内存：↓ 50%

对比：直接减少到8个注意力头
  ├─ 困惑度：↓ 5-10%  ← 明显衰退！
  └─ 这才是真正的性能损失

结论：GQA的0-2%损失 << 简单减头的5-10%损失
```

#### 为什么 GQA 损失这么小？

```
关键认识：K/V具有高冗余度

标准MHA的问题：
  32个完全独立的K/V投影
  ├─ 参数很多
  ├─ 但在语义上有重复
  └─ 许多K在特征空间中其实很相似

GQA的巧妙设计：
  ├─ 只用8个K/V头（减少参数）
  ├─ 配合32个多样的Q头来查询
  └─ 效果：用32种不同的"视角"看同一组信息
  
类比：
  MHA = 32本书 + 32副眼镜  （冗余）
  GQA = 8本书 + 32副眼镜   （高效！）
  
  关键信息全在，只是共享了"书"的内容
  但32副眼镜保证了解读的多样性
```

#### 生成长文本时的成本对比

```
生成1000个词的例子：

标准MHA（nhead=32）：
  KV Cache大小：
    1000(seqlen) × 32(nhead) × 128(d) × 4bytes
    = 16.4 MB
  推理计算：Q·K^T 的规模 = O(1000²)

GQA-8（nkvhead=8）：
  KV Cache大小：
    1000 × 8 × 128 × 4
    = 4.1 MB  ← 减少 75%！
  推理计算：Q(32)·K(8)^T，仍是线性复杂度
  
  省了什么：
  ├─ 内存：12.3 MB
  ├─ 推理速度：3倍快（更好的缓存局部性）
  └─ 代价：困惑度↑ 0-2%

投资回报率：
  投入：0.2% 困惑度损失
  收获：75% 内存减少 + 3倍速度
  
  这笔交易太值了！
```

#### 训练 vs 推理对 GQA 的影响不同

```
推理时（生成模式）：
  优势最大：
  ├─ KV Cache占用大量内存（线性增长）
  ├─ GQA能显著降低内存压力
  ├─ 允许生成更长的序列
  └─ 批量大小可以更大

训练时（完整序列一次性计算）：
  优势较小：
  ├─ K/V不需要缓存（直接全量计算）
  ├─ 内存节省不如推理明显
  ├─ 主要优势来自参数减少（1800万→1400万参数）
  └─ 性能影响取决于学习率等超参数调整

常见做法：
  ├─ 大模型：用标准MHA训练
  │  （参数越多效果越好，不怕麻烦）
  │
  ├─ 中等模型：用GQA训练和推理
  │  （平衡性能和成本）
  │
  └─ 端侧/移动部署：GQA推理+蒸馏
     （严格限制内存和速度）
```

#### 现实应用中的采用情况

```
DeepSeek、Qwen等前沿模型使用 GQA 的原因：

✓ 推理加速 2-3 倍
  ├─ 降低推理成本（降低GPU时间）
  └─ 用户获得更快响应

✓ KV Cache 减少 75%
  ├─ 允许更大的批量推理
  ├─ 更长的序列处理
  └─ 多用户并发推理

✓ 困惑度损失仅 0-2%
  ├─ 对最终应用体验无明显影响
  ├─ 通过更好的数据质量补偿
  └─ 整体收益远超成本

如果性能损失是 5-10%，就不值得了
但 0-2% 的损失换来 75% 内存 + 3倍速度？
这是现代LLM设计的必然选择！
```

---

## 六、与前面学过的内容的关联

### 6.1 RoPE + Self-Attention

```
RoPE 的作用（位置编码）：
  ├─ 在向量中嵌入相对位置信息
  ├─ 通过旋转保留向量长度
  ├─ 相对位置通过内积 Q·K^T 自动编码
  
Self-Attention 中的应用：
  q_rotated = rotate(q, pos_q)
  k_rotated = rotate(k, pos_k)
  
  A = q_rotated · k_rotated^T
    = |q| |k| cos(φ_q - φ_k)  ← 包含相对位置差 φ_q - φ_k
  
  结果：不同位置的词自动产生不同的注意力！
```

### 6.2 RMSNorm + Self-Attention

```
RMSNorm 的作用（归一化）：
  ├─ 稳定每个维度的大小
  ├─ 防止某维度过大导致梯度消失
  ├─ 让线性层(W_q、W_k、W_v)输入分布一致
  
Self-Attention 中的效果：
  x_normalized = rms_norm(x)  ← 分布稳定
  Q = x_normalized @ W_q      ← 投影更稳定
  K = x_normalized @ W_k
  V = x_normalized @ W_v
  
  A = Q·K^T / sqrt(d)          ← softmax输入范围稳定
  
  好处：梯度流更稳定，训练收敛更快
```

### 6.3 完整Transformer Block

```
x_input [seqlen, d_model]
  ↓
RMSNorm(x_input)  ← 稳定输入分布
  ↓
RoPE(normalized)  ← 添加位置信息
  ↓
Self-Attention(Q, K, V)  ← 聚合前文信息
  attn_output = softmax(Q·K^T / sqrt(d)) @ V
  ↓
残差连接 + RMSNorm
  ↓
FFN  ← 非线性变换
  ↓
残差连接
  ↓
输出 y_output [seqlen, d_model]
```

---

## 七、Self-Attention 的参数数量

```
单个Transformer层的Self-Attention部分：

W_q: [d_model, d] = [4096, 128] = 524,288 参数
W_k: [d_model, d] = [4096, 128] = 524,288 参数
W_v: [d_model, d_v] = [4096, 128] = 524,288 参数
输出投影：[4096, 4096] = 16,777,216 参数

小计：约1800万参数

一个完整Transformer块（含FFN）：可能有1亿+参数

整个大模型：可能有10亿-1000亿+参数，都通过反向传播训练！
```

---

## 八、常见问题

### Q1: 为什么要除以 sqrt(d)？

```
A = Q·K^T 的分布分析：

不除以sqrt(d)：
  如果d很大（比如128）
  Q、K向量的每一维都是[-1, 1]范围
  点积会累积，导致 A 的值变得很大
  
  例：d=128时，A 可能在 [-100, 100] 范围
  softmax([100, -100, 50]) 会导致：
    exp(100) ≈ 10^43  ← 爆炸！
    梯度接近0 ← 梯度消失！

除以sqrt(d)：
  A' = A / sqrt(128) ≈ A / 11.3
  A' 在 [-9, 9] 范围
  softmax 梯度正常，训练稳定！

一般规则：
  缩放因子 = 1 / sqrt(d)
  目的：让 Q·K^T 的方差 = 1，稳定softmax
```

### Q2: 多头是否都学相同的模式？

```
不同的头学习不同的关系模式：

Head 0 可能学到："词性匹配"
  "名词"关注"名词"，"动词"关注"动词"

Head 1 可能学到："语义相似性"
  "猫"关注"狗"、"动物"等语义相近词

Head 2 可能学到："距离感"
  关注距离较近的词（局部上下文）

...

最终输出：融合所有头的信息
attn_output = concat(head0_output, head1_output, ..., headN_output)
            = 包含多种关系的丰富表示
```

### Q3: 为什么 Self-Attention 能"学到"语义？

```
关键在于反向传播（训练时）：

初始状态：W_q、W_k 是随机的
→ A = Q·K^T 的相似度毫无意义
→ softmax后attention weights几乎随机
→ 模型生成的词错误

训练信号（梯度）：
  "为什么预测错了？"
  → 因为关注了错误的词
  → 我需要调整W_q和W_k

反向传播后：
  ∂loss/∂W_q 和 ∂loss/∂W_k 会指向让"相关词"相似的方向

多次迭代后：
  W_q 学会：在查询空间中突出"需要什么信息"
  W_k 学会：在键空间中也突出"是什么信息"
  → 语义相关的词自动产生高点积
  → attention weights 自动正确分配

这就是深度学习的魔法！
```

---

## 九、实现注意事项

### Self-Attention 函数签名

```cpp
void self_attention(
  tensor_t attn_val,    // 输出：[seqlen, nhead, dv]
  tensor_t q,           // 查询：[seqlen, nhead, d]
  tensor_t k,           // 键：[total_len, nkvhead, d]
  tensor_t v,           // 值：[total_len, nkvhead, dv]
  float scale           // 缩放因子：通常 1/sqrt(d)
);
```

### 计算步骤

1. **计算注意力分数**：`scores = Q @ K^T * scale`
2. **应用Causal Mask**：将位置 i > j 的分数设为 -∞
3. **Softmax**：行方向归一化
4. **加权求和**：`output = softmax_scores @ V`

### 多头处理

```
对于每个注意力头：
  分别计算 Q[head]、K[head]、V[head]
  执行上述操作
  最后拼接所有头的输出
```

### 内存索引

```
对于 3D 张量 [dim0, dim1, dim2]：
  linear_idx = i0 * (dim1 * dim2) + i1 * dim2 + i2
```

---

## 十、学习路线总结

```
Transformer 核心算子学习进度：

✅ RMSNorm
   └─ 作用：稳定激活，为后续操作创建一致的输入分布

✅ RoPE
   └─ 作用：编码相对位置，旋转不改变向量长度

🚧 Self-Attention  ← 当前
   └─ 作用：通过 Q·K^T 的相似度聚合前文信息

❌ Rearrange（张量重组）
❌ SwiGLU（激活函数）
❌ ...其他算子

完整推理流程（Assignment #3）：
  在这些算子基础上组装完整的 Transformer 推理
```

---

## 参考：公式速查表

| 公式 | 含义 |
|------|------|
| $Q = xW_q$ | 查询投影 |
| $K = xW_k$ | 键投影 |
| $V = xW_v$ | 值投影 |
| $A = \frac{QK^T}{\sqrt{d}}$ | 相似度矩阵（缩放） |
| $A_{masked} = A + \text{Mask}$ | 应用因果掩码 |
| $W = \text{softmax}(A_{masked})$ | 注意力权重 |
| $\text{output} = W \times V$ | 加权聚合 |

---

**下一步**：实现 self_attention 函数，验证与 PyTorch 参考实现一致。

