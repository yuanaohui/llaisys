# RoPE (Rotary Position Embedding) 知识笔记

## 📌 核心概念

**RoPE = 旋转位置编码**，是 Transformer 模型中的一种位置编码方式，通过**旋转变换**将位置信息编码到向量中。

---

## 1️⃣ RoPE 在大模型中的作用

### 在 Transformer 架构中的位置
- **位置**：每个 Transformer Block 内，在 Attention 层对 Q（Query）和 K（Key）进行处理
- **应用时机**：计算 Attention 之前，对 Q 和 K 向量进行旋转编码

### 核心功能
1. **编码位置信息**：让模型知道每个 token 在序列中的位置
2. **相对位置建模**：模型学习的是"相邻"、"间隔2个"等相对关系，而非"第5个token"这样的绝对位置
3. **长度外推**：训练时用 512 长度，推理时可以处理 2048+ 长度的序列
4. **稳定数值**：旋转保留向量长度，不破坏原始语义

### 训练/推理流程
```
训练阶段：
Input → Embedding → RoPE → Attention → FFN → ... → Output
               ↓ 
         让模型学习相对位置关系

推理阶段：
New Input → Embedding → RoPE → Attention → FFN → ... → Output
                   ↓
             自动泛化到新长度（长度外推）
```

---

## 2️⃣ 基本数学原理

### 数学公式

对输入向量 $x_i = [a_i, b_i] \in \mathbb{R}^d$（其中 $a_i, b_i \in \mathbb{R}^{d/2}$）：

**角度计算：**
$$\phi_{i,j} = \frac{p_i}{\theta^{2j/d}}$$

其中：
- $p_i$ = token 的位置 ID（从 pos_ids 获取）
- $\theta$ = 固定基数（通常 10000）
- $j$ = 维度索引（0, 1, ..., d/2-1）
- $d$ = 向量维度

**旋转变换：**
$$a'_{i,j} = a_{i,j} \cos(\phi_{i,j}) - b_{i,j} \sin(\phi_{i,j})$$
$$b'_{i,j} = b_{i,j} \cos(\phi_{i,j}) + a_{i,j} \sin(\phi_{i,j})$$

### 复数域解释

这是标准的 2D 旋转矩阵：
$$\begin{bmatrix} a' \\ b' \end{bmatrix} = \begin{bmatrix} \cos\phi & -\sin\phi \\ \sin\phi & \cos\phi \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix}$$

在复数域：$z' = z \cdot e^{i\phi}$（旋转复数）

---

## 3️⃣ 关键概念详解

### Q1：频率 (freq) 是什么？从哪里来？

**定义：** 频率 = 旋转速度

$$\text{freq}_j = \frac{1}{\theta^{2j/d}}$$

**来源：** 这是 RoPE 论文预先设计的公式，**不是计算出来的**。

**例子（d=4, theta=10000）：**
```
j=0: freq_0 = 1 / 10000^0     = 1.0    (快速旋转，高频)
j=1: freq_1 = 1 / 10000^0.5   = 0.01   (慢速旋转，低频)
```

**作用：**
- **高频**（freq 大）：捕捉短距离位置关系（相邻 token）
- **低频**（freq 小）：捕捉长距离位置关系（远距离 token）

**类比：** 就像时钟
- 秒针（高频）：快速转动，短时间内变化大
- 时针（低频）：缓慢转动，长时间才变化明显

---

### Q2：角度 (angle) 是什么？

**定义：** 角度 = 位置 × 频率

$$\text{angle}_j = \text{pos} \times \text{freq}_j$$

**例子：**
```
位置 0: angle = 0 × freq = 0     (不旋转)
位置 1: angle = 1 × freq = freq
位置 10: angle = 10 × freq        (旋转更多)
```

**物理意义：** 位置越大，旋转角度越大，向量方向变化越多。

---

### Q3：如何将位置信息与向量结合？

**关键：直接旋转向量本身，而非加法！**

#### 传统方法 vs RoPE

| 方法 | 操作 | 效果 |
|------|------|------|
| **传统 PE** | `x_new = x + PE(pos)` | ❌ 加法破坏原始语义 |
| **RoPE** | `x_new = Rotate(x, angle)` | ✅ 保留向量长度，只改变方向 |

#### 具体流程
```python
原始向量: [3.0, 4.0, 1.0, 0.0]  (d=4)
         ↓
分组: [3.0, 4.0] 和 [1.0, 0.0]
         ↓
分别旋转不同角度
         ↓
新向量: [0.779, 3.804, 3.063, 1.244]  (含位置信息)
```

**验证向量长度不变：**
```
|原始| = √(3²+4²+1²+0²) = √26 ≈ 5.099
|新的| = √(0.779²+3.804²+3.063²+1.244²) ≈ 5.099  ✅
```

---

### Q4：为什么要分组？

**原因：旋转是 2D 操作，需要成对的数据。**

- **1D 向量**：无法旋转（没有平面）
- **2D 向量**：可以在平面上旋转
- **高维向量**：分解成多个 2D 对，每对独立旋转

**类比：**
```
你无法旋转一条线（1D）
但可以旋转一个平面上的点（2D）
3D 物体可以分解为多个 2D 切面分别旋转
```

---

### Q5：如何分组？分组规则是什么？

**固定规则：前 d/2 个和后 d/2 个配对**

$$\text{第 } j \text{ 对} = [x_j, x_{j+d/2}], \quad j = 0, 1, ..., d/2-1$$

#### 例子 1：d=4
```
向量: [x₀, x₁, x₂, x₃]

第0对: [x₀, x₂]  → 用 freq_0 旋转
第1对: [x₁, x₃]  → 用 freq_1 旋转
```

#### 例子 2：d=8
```
向量: [x₀, x₁, x₂, x₃, x₄, x₅, x₆, x₇]

第0对: [x₀, x₄]  → 用 freq_0 旋转 (高频)
第1对: [x₁, x₅]  → 用 freq_1 旋转
第2对: [x₂, x₆]  → 用 freq_2 旋转
第3对: [x₃, x₇]  → 用 freq_3 旋转 (低频)
```

**为什么这样分组？**
- 让每对使用不同频率
- 实现多尺度位置编码
- 这是 RoPE 论文的设计选择

---

### Q6：分组后如何旋转？

**每一对使用自己的频率和角度独立旋转**

#### 标准旋转公式（第 j 对）

对 $[x_j, x_{j+d/2}]$：

$$x'_j = x_j \cos(\phi_j) - x_{j+d/2} \sin(\phi_j)$$
$$x'_{j+d/2} = x_{j+d/2} \cos(\phi_j) + x_j \sin(\phi_j)$$

其中：$\phi_j = \text{pos} \times \text{freq}_j$

#### 完整计算例子（d=8, pos=2, theta=10）

**步骤 1：计算频率**
```
freq_0 = 1.0
freq_1 = 0.562
freq_2 = 0.316
freq_3 = 0.178
```

**步骤 2：计算角度**
```
angle_0 = 2 × 1.0 = 2.0 rad
angle_1 = 2 × 0.562 = 1.124 rad
angle_2 = 2 × 0.316 = 0.632 rad
angle_3 = 2 × 0.178 = 0.356 rad
```

**步骤 3：分组旋转**
```
输入: [1, 2, 3, 4, 5, 6, 7, 8]

第0对 [1, 5] 用 2.0 rad 旋转 → [-4.961, -1.171]
第1对 [2, 6] 用 1.124 rad 旋转 → [-4.274, 4.204]
第2对 [3, 7] 用 0.632 rad 旋转 → [-1.704, 7.302]
第3对 [4, 8] 用 0.356 rad 旋转 → [1.488, 8.180]
```

**步骤 4：拼接结果**
```
输出: [-4.961, -4.274, -1.704, 1.488, -1.171, 4.204, 7.302, 8.180]
```

---

## 4️⃣ 为什么 RoPE 有效？

### 信息保留机制：旋转为什么能保存原始信息？

#### 向量的两个独立属性

向量包含两种信息：
- **幅度（长度）**：代表"强度"、"重要性" → **语义信息**
- **方向（角度）**：代表"指向" → **位置信息**

**RoPE 的巧妙之处：** 将这两种信息**完全分离**编码。

#### 旋转变换的数学性质

对任意 2D 向量 $[a, b]$ 进行旋转变换：

$$\begin{bmatrix} a' \\ b' \end{bmatrix} = \begin{bmatrix} \cos\phi & -\sin\phi \\ \sin\phi & \cos\phi \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix}$$

**关键性质：**
- ✅ **幅度（长度）完全不变**
- ⚠️ **方向改变了 $\phi$ 角度**

#### 数学证明

旋转后向量的长度：
$$|v'| = \sqrt{(a')^2 + (b')^2}$$

展开：
$$= \sqrt{(a\cos\phi - b\sin\phi)^2 + (b\cos\phi + a\sin\phi)^2}$$

$$= \sqrt{a^2\cos^2\phi - 2ab\cos\phi\sin\phi + b^2\sin^2\phi + b^2\cos^2\phi + 2ab\sin\phi\cos\phi + a^2\sin^2\phi}$$

$$= \sqrt{a^2(\cos^2\phi + \sin^2\phi) + b^2(\sin^2\phi + \cos^2\phi)}$$

利用三角恒等式 $\cos^2\phi + \sin^2\phi = 1$：
$$= \sqrt{a^2 + b^2} = |v|$$

**结论：旋转前后长度完全相同！** ✅

#### 直观例子

**例1：简单向量**
```
原始向量 [3, 4]:
  幅度 = √(3² + 4²) = √25 = 5
  方向 = arctan(4/3) ≈ 53.13°

旋转 90° 后变成 [-4, 3]:
  幅度 = √((-4)² + 3²) = √25 = 5  ✅ 相同
  方向 = arctan(3/-4) ≈ 143.13°   (增加了 90°)
```

**例2：向量 [1, 0]**
```
原始：幅度 = 1，方向 = 0°

旋转 45° → [0.707, 0.707]:
  幅度 = √(0.707² + 0.707²) ≈ 1  ✅
  
旋转 90° → [0, 1]:
  幅度 = √(0² + 1²) = 1  ✅
  
旋转 180° → [-1, 0]:
  幅度 = √((-1)² + 0²) = 1  ✅
```

**无论旋转多少角度，长度始终是 1！**

#### 信息分离的意义

| 属性 | 编码内容 | 变化规律 | 保留程度 |
|------|---------|---------|---------|
| **幅度** | 语义信息（重要性、强度） | 不变 | 100% 保留 ✅ |
| **方向** | 位置信息 | 随位置改变 | 精确编码 ✅ |

**对比传统加法编码：**
```
传统 PE: x_new = x + PE
  原始向量: [3, 4]，长度 = 5
  位置编码: [0.5, 0.5]
  结果: [3.5, 4.5]，长度 = √(3.5² + 4.5²) ≈ 5.7  ❌ 语义被破坏

RoPE: x_new = Rotate(x, φ)
  原始向量: [3, 4]，长度 = 5
  旋转 30°
  结果: [2.6, 4.5]，长度 = 5  ✅ 语义完美保留
```

#### 为什么这很重要？

在深度学习中：
- **向量长度**通常表示特征的"强度"或"置信度"
- **Attention 机制**中，内积计算既需要语义相似度，也需要位置关系
- **RoPE** 让模型同时获得两种信息，且互不干扰

**类比：指南针**
```
指针长度 = 指示强度（信号强弱）
指针方向 = 指示方位（东南西北）

旋转指针时：
- 指针长度不变 → 信号强度不变
- 指针方向改变 → 方位信息改变

两种信息完美分离！
```

---

### 核心机制：相对位置自动编码

在计算 Attention 时：
$$Q_{pos=i} \cdot K_{pos=j} \propto \cos(\phi_i - \phi_j)$$

**关键性质：**
- 内积自动包含角度差 $\Delta\phi = \phi_i - \phi_j$
- 角度差只依赖于**相对位置** $\Delta pos = i - j$
- 模型学习的是相对关系，而非绝对位置

### 例子：Attention 矩阵

3 个 token，原始向量都是 `[1, 0]`（语义相同）

```
旋转后：
pos=0: [1.0, 0.0]           (不旋转)
pos=1: [0.54, 0.84]         (旋转 1 rad)
pos=2: [-0.42, 0.91]        (旋转 2 rad)

Attention 内积矩阵 (Q·K^T)：
       k0     k1     k2
q0    1.00   0.54  -0.42
q1    0.54   1.00   0.54   ← 注意：相同相对位置 = 相同值
q2   -0.42   0.54   1.00
```

**观察：**
- 对角线全是 1.0（自己和自己）
- 相邻位置（±1）的值都是 0.54
- **相同的相对位置 → 相同的 attention 权重**

---

## 5️⃣ RoPE 的设计思路

### 背景：传统位置编码的问题

| 方法 | 问题 |
|------|------|
| **Sinusoidal PE** | 加法破坏语义，位置信息分散 |
| **Learned PE (BERT)** | 无法外推到训练长度之外 |
| **ALiBi** | 只能在 Attention 处理，不够通用 |

### 创新点

#### 1. 旋转替代加法
- **之前**：`x_new = x + PE(pos)` ❌
- **RoPE**：`x_new = Rotate(x, angle)` ✅

#### 2. 复数域的优雅
利用复数乘法的旋转性质：$z' = z \cdot e^{i\phi}$

#### 3. 自动的相对位置
Q·K^T 自动包含相对位置信息，无需显式计算

#### 4. 长度外推能力
相同相对位置的旋转角度差总是相同，无论绝对位置多大

### 灵感来源

1. **复数旋转**：数学上旋转保留向量长度
2. **傅里叶级数**：多频率分解信号（多尺度）
3. **相对位置编码**：只关心相对关系，不关心绝对位置

---

## 6️⃣ RoPE 的优势

### 与其他位置编码对比

| 特性 | 传统 PE | Learned PE | RoPE |
|------|---------|-----------|------|
| **语义保留** | ❌ 加法破坏 | ❌ 加法破坏 | ✅ 旋转保留 |
| **长度外推** | ⚠️ 有限 | ❌ 无法 | ✅ 自动 |
| **相对编码** | ❌ 间接 | ❌ 绝对 | ✅ 直接 |
| **额外参数** | ✅ 无 | ❌ 需要 | ✅ 无 |
| **多尺度** | ✅ 有 | ❌ 无 | ✅ 有 |
| **数学优雅** | ⚠️ 启发式 | ❌ 工程化 | ✅ 理论基础 |

### 实际效果

1. **训练稳定性好**：不破坏向量语义
2. **长度外推强**：训练 512，推理 2048+ 无问题
3. **计算高效**：仅需三角函数，易并行化
4. **广泛应用**：LLaMA、Qwen、GLM 等主流模型都用 RoPE

---

## 7️⃣ 实现伪代码

```python
def rope(x, pos, theta=10000):
    """
    x: 向量 [d]
    pos: 位置 ID
    theta: 基数
    """
    d = len(x)
    
    for j in range(d // 2):
        # 计算频率和角度
        freq_j = 1.0 / (theta ** (2 * j / d))
        angle_j = pos * freq_j
        
        # 获取第 j 对
        a = x[j]           # 前半部分的第 j 个
        b = x[j + d // 2]  # 后半部分的第 j 个
        
        # 旋转变换
        cos_val = cos(angle_j)
        sin_val = sin(angle_j)
        
        x[j]         = a * cos_val - b * sin_val
        x[j + d // 2] = b * cos_val + a * sin_val
    
    return x
```

---

## 8️⃣ 关键参数

### out, in, pos_ids

- **in**：输入张量，shape `[seqlen, nhead, d]`
- **out**：输出张量，shape `[seqlen, nhead, d]`
- **pos_ids**：位置 ID，shape `[seqlen]`，dtype `int64`
- **theta**：基数，通常 10000

### 三重循环结构

```cpp
for (seq = 0; seq < seqlen; ++seq) {
    int64_t pos = pos_ids[seq];  // 获取位置
    
    for (head = 0; head < nhead; ++head) {
        
        for (j = 0; j < d/2; ++j) {
            // 计算频率和角度
            float freq = 1.0f / powf(theta, 2.0f * j / d);
            float angle = pos * freq;
            
            // 旋转第 j 对
            float a = in[seq][head][j];
            float b = in[seq][head][j + d/2];
            
            out[seq][head][j]       = a * cos(angle) - b * sin(angle);
            out[seq][head][j + d/2] = b * cos(angle) + a * sin(angle);
        }
    }
}
```

---

## 9️⃣ 常见问题 FAQ

### Q: 为什么叫"旋转"位置编码？
A: 因为数学上是 2D 旋转矩阵，在复数域就是 $z \cdot e^{i\phi}$。

### Q: 旋转为什么可以保存原始信息？向量不是改变了吗？
A: 旋转只改变**方向**，不改变**幅度（长度）**。语义信息存储在向量的幅度中，位置信息编码在方向的变化中。

**验证公式：** 旋转后的向量长度 $|v'| = \sqrt{(a')^2 + (b')^2} = \sqrt{a^2+b^2} = |v|$

**例子：**
- 原始：`[3, 4]` → 长度 = 5（语义：重要性）
- 旋转90°：`[-4, 3]` → 长度 = 5（语义保留✅），方向改变（位置信息）

### Q: RoPE 如何知道 Q 和 K 的相对位置？
A: 通过**内积运算自动提取角度差**。

**数学原理：**
$$Q_i \cdot K_j = \cos(\phi_i - \phi_j) = \cos((i-j) \times \text{freq})$$

内积只依赖于**相对位置差 $(i-j)$**，与绝对位置无关。

**例子：**
- Q在位置1，K在位置3：内积 = $\cos(-2 \times \text{freq})$
- Q在位置10，K在位置12：内积 = $\cos(-2 \times \text{freq})$（相同！）

模型学到的是"相距2个位置的token应该有多大的attention"，而非"位置1和3"这样的绝对关系。

### Q: RoPE 是如何一开始就知道 Q 和 K 在什么位置的？
A: **通过显式传入的 `pos_ids` 参数**。

RoPE 本身**不会自动知道位置**，而是需要外部告诉它每个 token 的位置信息。

**具体流程：**
1. **输入数据**：有一个序列 `["我", "爱", "中国"]`
2. **位置标记**：系统给每个 token 分配位置 ID：`[0, 1, 2]`
3. **传入 RoPE**：
   ```python
   rope(in, pos_ids=[0, 1, 2], ...)
   ```
4. **RoPE 处理**：
   - 对位置 0 的 token：旋转角度 = 0 × freq = 0
   - 对位置 1 的 token：旋转角度 = 1 × freq
   - 对位置 2 的 token：旋转角度 = 2 × freq

**pos_ids 的来源：**
- **训练/推理时**：通常是简单的序列 `[0, 1, 2, ..., seqlen-1]`
- **特殊情况**：可以自定义（如处理填充、缓存等场景）

**示例代码：**
```python
# Transformer 内部
def forward(self, input_ids):
    # 1. 获取序列长度
    seqlen = input_ids.shape[1]
    
    # 2. 生成位置 ID
    pos_ids = torch.arange(seqlen)  # [0, 1, 2, ..., seqlen-1]
    
    # 3. Embedding
    x = self.embedding(input_ids)
    
    # 4. 应用 RoPE（显式传入位置）
    q = rope(q, pos_ids)
    k = rope(k, pos_ids)
    
    # 5. 计算 Attention
    attn = q @ k.T
```

**关键点：**
- pos_ids 是**外部输入**，不是 RoPE 内部计算的
- RoPE 只负责"根据给定的位置 ID 进行旋转"
- 位置信息的获取由调用者（Transformer 框架）负责

### Q: 为什么能长度外推？
A: 因为学习的是相对位置关系。相对距离 Δ=1 在任何绝对位置都是一样的旋转角度差。

### Q: 为什么多个频率？
A: 不同频率捕捉不同尺度的位置关系（短距离 vs 长距离）。

### Q: theta=10000 是怎么定的？
A: 经验值，论文设定。太小会导致长序列时角度重复，太大会导致短序列时分辨率不够。

### Q: 能用于其他模态（图像/音频）吗？
A: 可以，但需要调整（如 2D RoPE for Vision Transformer）。

### Q: 如果向量维度是奇数怎么办？
A: **RoPE 要求维度必须是偶数**，这是设计上的硬性约束。

**原因：**
- 旋转是 2D 操作，必须成对进行
- 每对需要 2 个元素：$[x_j, x_{j+d/2}]$
- 如果 $d$ 是奇数，无法完美配对

**实际情况：**
在所有使用 RoPE 的大模型中，隐藏层维度都是偶数：
- **LLaMA**: 4096, 5120, 6656（偶数）
- **Qwen**: 4096, 8192（偶数）
- **GPT**: 768, 1024, 1536, 2048（偶数）

**如果真的遇到奇数维度：**
1. **填充到偶数**（最简单）
   ```
   原始维度 d=127
   填充一个 0 → d'=128
   RoPE(前128维) + 保留最后一维不变
   ```

2. **降维到偶数**（不推荐）
   ```
   d=127 → 丢弃最后一维 → d'=126
   ```

3. **只对前 d-1 维应用 RoPE**
   ```
   d=127
   对前 126 维应用 RoPE（63对）
   最后 1 维保持不变
   ```

**为什么模型设计者总选偶数维度？**
- 便于 RoPE 等位置编码
- 便于并行计算（2的幂次更优）
- 便于矩阵分块（多头注意力）

**验证代码：**
```python
# 在 RoPE 实现中通常有这样的检查
assert d % 2 == 0, f"RoPE requires even dimension, got {d}"
```

**结论：** RoPE 的设计就是为偶数维度优化的，实践中不会遇到奇数维度的情况。

---

## 🔟 时间线与应用

```
2017: Transformer 原论文（Sinusoidal PE）
      ↓
2021: RoPE 论文发布（RoFormer）
      ↓
2023: LLaMA、Qwen 等大模型广泛采用
      ↓
2024: 成为事实标准位置编码
```

### 主流应用模型
- **LLaMA 系列**（Meta）
- **Qwen 系列**（阿里）
- **GLM 系列**（智谱）
- **DeepSeek 系列**

---

## 📝 总结

### 一句话总结
**RoPE 通过复数旋转，将位置信息优雅地编码到向量方向中，实现了相对位置建模和长度外推。**

### 核心优势
1. ✅ 保留向量语义（旋转不改变幅度）
2. ✅ 自动相对位置编码
3. ✅ 长度外推能力强
4. ✅ 数学优雅、实现简单
5. ✅ 无额外参数、计算高效

### 为什么重要？
在大模型时代，RoPE 解决了位置编码的核心痛点（长度外推），成为了 Transformer 架构的标准组件。

---

## 📚 参考资料

- **论文**：Su et al., "RoFormer: Enhanced Transformer with Rotary Position Embedding" (2021)
- **应用**：LLaMA, Qwen, GLM 等模型的技术报告
- **实现**：本项目 `src/ops/rope/op.cpp`

---

*笔记整理时间：2026年2月3日*  
*基于 LLAISYS 项目学习经历*
